# backend/routes/voice.py
"""
Route pour le canal Voix (Vapi) - DEBUG COMPLET + TIMERS
Avec m√©moire client et stats pour rapports.
"""

from fastapi import APIRouter, Request
from fastapi.responses import Response, JSONResponse, StreamingResponse
import asyncio
import logging
import json
import re
import time
import uuid
from typing import Optional, TYPE_CHECKING

from backend.engine import ENGINE
from backend import prompts, config
from backend.client_memory import get_client_memory
from backend.session_codec import session_to_dict
from backend.conversational_engine import ConversationalEngine, _is_canary
from backend.reports import get_report_generator
from backend.stt_utils import normalize_transcript, is_filler_only
from backend.stt_common import (
    classify_text_only,
    estimate_tts_duration,
    is_critical_overlap,
    is_critical_token,
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Instances singleton
client_memory = get_client_memory()
report_generator = get_report_generator()

# Mode conversationnel P0 (lazy)
_conversational_engine = None

def _get_or_resume_voice_session(tenant_id: int, call_id: str):
    """
    Phase 2: PG-first read pour reprise apr√®s restart/multi-instance.
    Si session absente en m√©moire ‚Üí tenter load depuis PG, sinon get_or_create.
    """
    session = ENGINE.session_store.get(call_id)
    if session is None and config.USE_PG_CALL_JOURNAL:
        try:
            from backend.session_pg import load_session_pg_first
            result = load_session_pg_first(tenant_id, call_id)
            if result:
                s_pg, ck_seq, last_seq = result
                if hasattr(ENGINE.session_store, "set_for_resume"):
                    ENGINE.session_store.set_for_resume(s_pg)
                else:
                    cache = getattr(ENGINE.session_store, "_memory_cache", None)
                    if cache is not None:
                        cache[call_id] = s_pg
                logger.info(
                    "[CALL_RESUME] source=pg tenant_id=%s call_id=%s state=%s ck_seq=%s last_seq=%s",
                    tenant_id, call_id[:20], s_pg.state, ck_seq, last_seq,
                )
                try:
                    from backend.engine import _persist_ivr_event
                    _persist_ivr_event(s_pg, "resume_from_pg", reason=f"ck_seq={ck_seq}")
                except Exception:
                    pass
                session = s_pg
        except Exception as e:
            logger.warning("[CALL_RESUME_WARN] pg_down/err=%s", e, exc_info=True)
    if session is None:
        session = ENGINE.session_store.get_or_create(call_id)
    return session


def _get_engine(call_id: str):
    """Retourne l'engine √† utiliser : conversationnel (si flag + canary) ou FSM."""
    if config.CONVERSATIONAL_MODE_ENABLED and _is_canary(call_id):
        global _conversational_engine
        if _conversational_engine is None:
            from backend.cabinet_data import CabinetData
            from backend.llm_conversation import get_default_conv_llm_client
            _conversational_engine = ConversationalEngine(
                cabinet_data=CabinetData.default(config.BUSINESS_NAME),
                faq_store=ENGINE.faq_store,
                llm_client=get_default_conv_llm_client(),
                fsm_engine=ENGINE,
            )
        return _conversational_engine
    return ENGINE


def _parse_stream_flag(payload: dict) -> bool:
    """
    D√©tection robuste de stream: true (Vapi Custom LLM).
    - bool ‚Üí ok
    - string "true"/"1"/"yes" (case-insensitive) ‚Üí True
    - string "false"/"0"/"no" ‚Üí False
    - int 1 ‚Üí True, 0 ‚Üí False
    - sinon ‚Üí bool(val) pour compat
    """
    for key in ("stream", "streaming"):
        val = payload.get(key)
        if val is None:
            continue
        if isinstance(val, bool):
            return val
        if isinstance(val, str):
            low = val.strip().lower()
            if low in ("true", "1", "yes"):
                return True
            if low in ("false", "0", "no"):
                return False
        if isinstance(val, int):
            return val != 0
        return bool(val)
    return False


def _make_chat_response(call_id: str, text: str, is_streaming: bool):
    """
    Point de sortie unique pour /chat/completions : SSE si stream demand√©, sinon JSON.
    Contrat Vapi : stream=true ‚Üí Content-Type text/event-stream + data: ... + data: [DONE].
    """
    if is_streaming:
        return StreamingResponse(
            _sse_stream_for_text(call_id, text or ""),
            media_type="text/event-stream",
            headers={"Cache-Control": "no-cache", "Connection": "keep-alive"},
        )
    return _chat_completion_response(call_id, text, _stream_requested=False)


def _sse_stream_for_text(call_id: str, text: str):
    """
    G√©n√©rateur SSE au format OpenAI chat.completion.chunk pour un texte complet.
    Utilis√© quand stream=true pour LockTimeout, erreur, ou r√©ponse courte.
    """
    chunk_role = {
        "id": f"chatcmpl-{call_id}",
        "object": "chat.completion.chunk",
        "choices": [{"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}],
    }
    yield f"data: {json.dumps(chunk_role)}\n\n"
    words = (text or "").strip().split()
    for i, word in enumerate(words):
        content = f" {word}" if i > 0 else word
        chunk = {
            "id": f"chatcmpl-{call_id}",
            "object": "chat.completion.chunk",
            "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}],
        }
        yield f"data: {json.dumps(chunk)}\n\n"
    chunk_final = {
        "id": f"chatcmpl-{call_id}",
        "object": "chat.completion.chunk",
        "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
    }
    yield f"data: {json.dumps(chunk_final)}\n\n"
    yield "data: [DONE]\n\n"


def _reconstruct_session_from_history(session, messages: list, call_id: str = ""):
    """
    Reconstruit l'√©tat de la session depuis l'historique des messages.
    N√©cessaire si la session en m√©moire a √©t√© perdue (red√©marrage Railway).
    
    STRAT√âGIE: Extraire TOUTES les donn√©es depuis l'historique
    Tra√ßabilit√©: log WARN, ivr_event session_reconstruct_used
    """
    from backend.guards import clean_name_from_vocal
    from backend.engine import _persist_ivr_event

    logger.warning(
        "[SESSION_RECONSTRUCT] conv_id=%s reason=session_lost messages=%s",
        call_id or getattr(session, "conv_id", ""),
        len(messages),
    )
    try:
        _persist_ivr_event(session, "session_reconstruct_used", reason="session_lost")
    except Exception:
        pass

    # Patterns pour d√©tecter l'√©tat
    patterns = {
        "QUALIF_NAME": ["c'est √† quel nom", "quel nom", "votre nom"],
        "QUALIF_PREF": ["matin ou l'apr√®s-midi", "matin ou apr√®s-midi", "pr√©f√©rez"],
        "QUALIF_CONTACT": ["num√©ro de t√©l√©phone", "t√©l√©phone pour vous rappeler", "redonner votre num√©ro"],
        "CONTACT_CONFIRM": ["votre num√©ro est bien", "j'ai not√© le", "je confirme", "c'est bien √ßa", "est-ce correct"],
        "WAIT_CONFIRM": ["j'ai trois cr√©neaux", "voici trois cr√©neaux", "j'ai deux cr√©neaux", "j'ai un cr√©neau", "dites un, deux ou trois", "dites simplement", "dites un ou deux"],
        "CONFIRMED": ["rendez-vous est confirm√©", "c'est confirm√©"],
        "POST_FAQ": ["puis-je vous aider pour autre chose", "autre chose pour vous", "souhaitez-vous autre chose"],
        "POST_FAQ_CHOICE": [
            "rendez-vous ou",
            "souhaitez-vous prendre rendez-vous",
            "ou avez-vous une autre question",
            "rdv ou question",
        ],
    }
    
    # Parcourir TOUS les messages pour extraire les donn√©es
    for i, msg in enumerate(messages):
        if msg.get("role") == "assistant":
            content = msg.get("content", "").lower()
            
            # Extraire le nom
            if any(p in content for p in patterns["QUALIF_NAME"]):
                if i + 1 < len(messages) and messages[i + 1].get("role") == "user":
                    potential_name = messages[i + 1].get("content", "").strip()
                    if (len(potential_name) >= 2 and 
                        len(potential_name) <= 50 and
                        "matin" not in potential_name.lower() and
                        "apr√®s" not in potential_name.lower()):
                        cleaned_name = clean_name_from_vocal(potential_name)
                        if len(cleaned_name) >= 2:
                            session.qualif_data.name = cleaned_name
                            logger.debug("reconstruct name: %r -> %r", potential_name, cleaned_name)
            
            # Extraire la pr√©f√©rence
            if any(p in content for p in patterns["QUALIF_PREF"]):
                if i + 1 < len(messages) and messages[i + 1].get("role") == "user":
                    potential_pref = messages[i + 1].get("content", "").strip()
                    if potential_pref and len(potential_pref) <= 50:
                        session.qualif_data.pref = potential_pref
                        logger.debug("reconstruct pref: %r", potential_pref)
            
            # Extraire le contact
            if any(p in content for p in patterns["QUALIF_CONTACT"]):
                if i + 1 < len(messages) and messages[i + 1].get("role") == "user":
                    potential_contact = messages[i + 1].get("content", "").strip()
                    if potential_contact:
                        session.qualif_data.contact = potential_contact
                        logger.debug("reconstruct contact: %r", potential_contact)

            # Extraire le choix de cr√©neau (1/2/3) apr√®s proposition de slots
            if any(p in content for p in patterns["WAIT_CONFIRM"]):
                if i + 1 < len(messages) and messages[i + 1].get("role") == "user":
                    choice_text = (messages[i + 1].get("content", "") or "").strip().lower()
                    choice_map = {"un": 1, "1": 1, "une": 1, "deux": 2, "2": 2, "trois": 3, "3": 3}
                    for k, v in choice_map.items():
                        if k in choice_text or choice_text == k:
                            session.pending_slot_choice = v
                            logger.debug("reconstruct pending_slot_choice: %r -> %s", choice_text, v)
                            break
    
    # D√©terminer l'√©tat ACTUEL bas√© sur le dernier message assistant
    last_assistant_msg = ""
    for msg in reversed(messages):
        if msg.get("role") == "assistant":
            last_assistant_msg = msg.get("content", "").lower()
            break
    
    detected_state = None
    for state, state_patterns in patterns.items():
        if any(p in last_assistant_msg for p in state_patterns):
            detected_state = state
            break
    
    # Si √©tat d√©tect√©
    if detected_state:
        session.state = detected_state
        logger.debug("reconstruct state: %s", detected_state)
        if detected_state == "WAIT_CONFIRM":
            logger.debug("reconstruct WAIT_CONFIRM - slots will be re-fetched on next handler call")
        # P0: CONTACT_CONFIRM sans slots ‚Üí re-fetch pour √©viter "probl√®me technique"
        if detected_state == "CONTACT_CONFIRM" and session.pending_slot_choice and not (getattr(session, "pending_slots", None) or []):
            try:
                from backend import tools_booking
                from backend.calendar_adapter import get_calendar_adapter
                fresh_slots = tools_booking.get_slots_for_display(limit=3, pref=getattr(session.qualif_data, "pref", None), session=session)
                if fresh_slots:
                    adapter = get_calendar_adapter(session)
                    source = "google" if (adapter and adapter.can_propose_slots()) else "sqlite"
                    session.pending_slots = tools_booking.to_canonical_slots(fresh_slots, source)
                    logger.info("[RECONSTRUCT_SLOTS] conv_id=%s re-fetched %s slots for CONTACT_CONFIRM", call_id or getattr(session, "conv_id", ""), len(fresh_slots))
            except Exception as e:
                logger.warning("[RECONSTRUCT_SLOTS] failed: %s", e)
    else:
        logger.warning("reconstruct could not detect state from last assistant msg")
    logger.debug("reconstruct complete: state=%s name=%s pref=%s", session.state, session.qualif_data.name, session.qualif_data.pref)
    
    return session


def _pg_lock_ok() -> bool:
    """Phase 2.1: PG journal activ√© et URL pr√©sente."""
    if not getattr(config, "USE_PG_CALL_JOURNAL", True):
        return False
    try:
        from backend.session_pg import _pg_url
        return _pg_url() is not None
    except Exception:
        return False


def _call_journal_ensure(tenant_id: int, call_id: str, initial_state: str = "START") -> None:
    """Phase 1 dual-write: assure call_sessions existe. Si PG down: log WARN, continue."""
    if not getattr(config, "USE_PG_CALL_JOURNAL", True):
        return
    try:
        from backend.session_pg import pg_ensure_call_session
        ok = pg_ensure_call_session(tenant_id, call_id, initial_state)
        if not ok:
            logger.debug("[CALL_JOURNAL] pg_ensure_call_session skipped (no PG)")
    except Exception as e:
        logger.warning("[CALL_JOURNAL_WARN] pg_down reason=ensure %s", e)


def _call_journal_user_message(tenant_id: int, call_id: str, text: str) -> None:
    """Phase 1 dual-write: log message user. Si PG down: log WARN, continue."""
    if not getattr(config, "USE_PG_CALL_JOURNAL", True):
        return
    try:
        from backend.session_pg import pg_ensure_call_session, pg_add_message
        pg_ensure_call_session(tenant_id, call_id)
        pg_add_message(tenant_id, call_id, "user", text or "")
    except Exception as e:
        logger.warning("[CALL_JOURNAL_WARN] pg_down reason=user_msg %s", e)


def _call_journal_agent_response(
    tenant_id: int,
    call_id: str,
    session,
    response_text: str,
    state_before: str,
    should_checkpoint: bool,
) -> None:
    """
    Phase 1 dual-write: log message agent, update state, optionnel checkpoint.
    should_checkpoint: True si state chang√© OU pending_slots critique OU toutes les N √©critures.
    """
    if not getattr(config, "USE_PG_CALL_JOURNAL", True):
        return
    try:
        from backend.session_pg import (
            pg_ensure_call_session,
            pg_add_message,
            pg_update_last_state,
            pg_write_checkpoint,
        )
        from backend.session_codec import session_to_dict
        pg_ensure_call_session(tenant_id, call_id)
        seq = pg_add_message(tenant_id, call_id, "agent", response_text or "")
        pg_update_last_state(tenant_id, call_id, getattr(session, "state", "START"))
        if should_checkpoint and seq is not None:
            state_json = session_to_dict(session)
            pg_write_checkpoint(tenant_id, call_id, seq, state_json)
    except Exception as e:
        logger.warning("[CALL_JOURNAL_WARN] pg_down reason=agent_response %s", e)


def log_timer(label: str, start: float) -> float:
    """Log le temps √©coul√© et retourne le nouveau timestamp."""
    now = time.time()
    elapsed_ms = (now - start) * 1000
    logger.debug("%s: %.0fms", label, elapsed_ms)
    return now


def _looks_like_name_for_cancel(text: str) -> bool:
    """True si le message ressemble √† un nom (annulation) : non vide, >= 2 car., pas que des chiffres."""
    if not text or not text.strip():
        return False
    t = text.strip()
    if len(t) < 2:
        return False
    if t.isdigit():
        return False
    return True


router = APIRouter(prefix="/api/vapi", tags=["voice"])


@router.get("/test-calendar")
async def test_calendar_connection():
    """Test de connexion Google Calendar"""
    from backend import tools_booking
    import os
    
    try:
        # Test 1: Variables d'environnement (lecture directe)
        env_var = os.getenv("GOOGLE_SERVICE_ACCOUNT_BASE64")
        calendar_id = os.getenv("GOOGLE_CALENDAR_ID", "6fd8676f...")
        
        # V√©rifier quel fichier est r√©ellement utilis√©
        from backend import config
        
        result = {
            "calendar_id": calendar_id,
            "service_account_file_from_config": config.SERVICE_ACCOUNT_FILE,
            "env_var_present": bool(env_var),
            "env_var_length": len(env_var) if env_var else 0,
            "file_exists": False,
            "slots_available": False,
            "error": None
        }
        
        # Test 2: Fichier existe ?
        import os
        result["service_account_file"] = config.SERVICE_ACCOUNT_FILE
        if config.SERVICE_ACCOUNT_FILE and os.path.exists(config.SERVICE_ACCOUNT_FILE):
            result["file_exists"] = True
        else:
            result["file_exists"] = False
            result["file_path_checked"] = config.SERVICE_ACCOUNT_FILE
        
        # Test 3: R√©cup√©rer des cr√©neaux
        slots = tools_booking.get_slots_for_display(limit=3)
        if slots and len(slots) > 0:
            result["slots_available"] = True
            result["slots"] = [{"idx": s.idx, "label": s.label} for s in slots]
        
        return result
        
    except Exception as e:
        import traceback
        return {
            "error": str(e),
            "traceback": traceback.format_exc()
        }




def _is_agent_speaking(session) -> bool:
    """Vrai si l'agent est en train de parler (TTS en cours, selon estimation)."""
    now = time.time()
    until = getattr(session, "speaking_until_ts", 0) or 0
    return now < until


def _parse_stream_flag(payload: dict) -> bool:
    """
    D√©tection robuste de stream/streaming dans le payload Vapi.
    - bool ‚Üí tel quel
    - string ‚Üí "true"/"1"/"yes" (case-insensitive) = True, "false"/"0"/"no" = False
    - int ‚Üí 1 = True, 0 = False
    - sinon ‚Üí bool(val) pour √©viter "false" string ‚Üí True
    """
    for key in ("stream", "streaming"):
        val = payload.get(key)
        if val is None:
            continue
        if isinstance(val, bool):
            return val
        if isinstance(val, str):
            if val.strip().lower() in ("true", "1", "yes"):
                return True
            if val.strip().lower() in ("false", "0", "no"):
                return False
        if isinstance(val, int):
            return val != 0
    return False


def _make_chat_response(call_id: str, text: str, is_streaming: bool):
    """
    Point de sortie unique pour /chat/completions : SSE si stream demand√©, sinon JSON.
    Contrat Vapi : stream=true ‚Üí Content-Type: text/event-stream + data: ... + data: [DONE].
    """
    if is_streaming:
        return StreamingResponse(
            _sse_stream_for_text(call_id, text),
            media_type="text/event-stream",
            headers={"Cache-Control": "no-cache", "Connection": "keep-alive"},
        )
    return _chat_completion_response(call_id, text, _stream_requested=False)


def _chat_completion_response(call_id: str, content: str, _stream_requested: bool = False):
    """
    R√©ponse OpenAI-like robuste pour /api/vapi/chat/completions (JSON uniquement).
    Compatibilit√© max : id, object, created, model, usage, content √† la racine, choices[0].text.
    Si VAPI_DEBUG_TEST_AUDIO=1 : force content = "TEST AUDIO 123" pour tester le pipeline TTS.
    _stream_requested: si True, log [STREAM_MISMATCH_GUARD] (chemin aurait d√ª renvoyer du SSE).
    """
    if _stream_requested:
        logger.warning(
            "[STREAM_MISMATCH_GUARD] call_id=%s route=chat/completions reason=json_returned_while_stream_requested",
            call_id[:24] if call_id else "n/a",
        )
    if getattr(config, "VAPI_DEBUG_TEST_AUDIO", False):
        content = "TEST AUDIO 123"
        logger.info("[VAPI_DEBUG] TEST AUDIO 123 forced for TTS check")
    text = (content or "").strip() or "Pouvez-vous r√©p√©ter, s'il vous pla√Æt ?"
    body = {
        "id": f"chatcmpl-{call_id}",
        "object": "chat.completion",
        "created": int(time.time()),
        "model": "uwi-agent",
        "content": text,
        "choices": [
            {
                "index": 0,
                "message": {"role": "assistant", "content": text},
                "text": text,
                "finish_reason": "stop",
            }
        ],
        "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0},
    }
    logger.info("[VAPI_OUT] chat/completions content_len=%s", len(text))
    return JSONResponse(
        body,
        status_code=200,
        headers={"Content-Type": "application/json; charset=utf-8"},
    )


def _vapi_content_response(
    call_id: str,
    response_text: str,
    debug_trace: Optional[str] = None,
    session=None,
):
    """
    R√©ponse JSON explicite pour Vapi : strip/fallback, log [VAPI_OUT], Content-Type application/json.
    Si session fourni : _debug = call_id[:6]|state (pour v√©rifier dans les logs Vapi que c'est cette route).
    """
    text = (response_text or "").strip()
    if not text:
        text = "Pouvez-vous r√©p√©ter, s'il vous pla√Æt ?"
    payload = {"content": text}
    if session is not None:
        payload["_debug"] = f"{call_id[:8]}|{getattr(session, 'state', '?')}"
    elif debug_trace:
        payload["_debug"] = debug_trace
    logger.info(
        "[VAPI_OUT] status=200 content_type=application/json content_len=%s call_id=%s _debug=%s",
        len(text), call_id[:20] if call_id else "n/a", payload.get("_debug", ""),
    )
    return JSONResponse(payload, status_code=200)


def _log_decision_out(
    call_id: str,
    session,
    action_taken: str,
    reply_text: str = "",
    session_key: Optional[str] = None,
) -> None:
    """Log d√©cisionnel sortie (sans PII). session_key = cl√© de session stable (pour debug boucle)."""
    logger.info(
        "decision_out",
        extra={
            "call_id": call_id,
            "session_key": session_key or call_id,
            "action": action_taken,
            "state_after": getattr(session, "state", "") if session else "",
            "reply_len": len(reply_text or ""),
        },
    )


def _maybe_reset_noise_on_terminal(session, events) -> None:
    """P1-1 : Reset compteurs noise + unclear quand on entre en √©tat terminal (session propre)."""
    if not session or not events:
        return
    conv_state = getattr(events[0], "conv_state", None)
    if conv_state in ("CONFIRMED", "TRANSFERRED"):
        session.noise_detected_count = 0
        session.last_noise_ts = None
        session.unclear_text_count = 0


def _classify_stt_input(
    raw_text: str,
    confidence: Optional[float],
    transcript_type: str,
    message_type: Optional[str] = None,
) -> tuple[str, str]:
    """
    Classifie l'entr√©e STT pour nova-2-phonecall.
    Returns: ("NOISE" | "SILENCE" | "TEXT", text_to_use)
    """
    if transcript_type == "partial":
        return "TEXT", raw_text  # fallback (ne devrait pas arriver ici si on filtre en amont)

    normalized = normalize_transcript(raw_text)

    # Whitelist : tokens critiques = TEXT m√™me si confidence tr√®s basse
    if is_critical_token(normalized):
        return "TEXT", normalized

    # Transcript vide
    if not normalized or not normalized.strip():
        if confidence is not None and confidence < config.NOISE_CONFIDENCE_THRESHOLD:
            return "NOISE", ""
        # P1 : Parole d√©tect√©e mais pas transcrite (pas de confidence) ‚Üí bruit probable
        if message_type:
            mt_lower = message_type.lower()
            if any(x in mt_lower for x in ("user-message", "audio", "speech", "detected")):
                return "NOISE", ""
        return "SILENCE", ""

    # Transcript tr√®s court ou filler seul
    if len(normalized) < config.MIN_TEXT_LENGTH or is_filler_only(normalized):
        if confidence is not None and confidence < config.SHORT_TEXT_MIN_CONFIDENCE:
            return "NOISE", normalized
    return "TEXT", normalized


@router.post("/webhook")
async def vapi_webhook(request: Request):
    """
    Webhook Vapi ‚Äî Option A : 200 imm√©diat, z√©ro traitement.
    √âvite la saturation du worker Railway : pas de request.json(), pas de log, pas de DB.
    Les events Vapi sont fire-and-forget ; le flux conversationnel passe par /chat/completions.
    """
    return Response(status_code=200)


@router.post("/tool")
async def vapi_tool(request: Request):
    """
    Endpoint pour Vapi Tools/Functions.
    Claude appelle ce tool pour obtenir les r√©ponses.
    """
    try:
        payload = await request.json()

        print(f"üîßüîßüîß TOOL APPEL√â üîßüîßüîß")
        print(f"üì¶ Payload: {json.dumps(payload, indent=2, ensure_ascii=False)}")

        # Extraire le message utilisateur
        user_message = payload.get("parameters", {}).get("user_message", "")
        call_id = payload.get("call", {}).get("id", "unknown")

        print(f"üìù User message: '{user_message}'")
        print(f"üìû Call ID: {call_id}")

        if not user_message:
            return {"result": "Je n'ai pas compris. Pouvez-vous r√©p√©ter ?"}

        # DID ‚Üí tenant_id (tool utilise same payload)
        from backend.tenant_routing import (
            extract_to_number_from_vapi_payload,
            resolve_tenant_id_from_vocal_call,
        )
        to_number = extract_to_number_from_vapi_payload(payload)
        resolved_tenant_id, _ = resolve_tenant_id_from_vocal_call(to_number, channel="vocal")

        # Phase 2.1: lock PG anti webhooks simultan√©s
        if _pg_lock_ok():
            try:
                from backend.session_pg import pg_lock_call_session, LockTimeout
                _call_journal_ensure(resolved_tenant_id, call_id)
                with pg_lock_call_session(resolved_tenant_id, call_id, timeout_seconds=2):
                    session = _get_or_resume_voice_session(resolved_tenant_id, call_id)
                    session.channel = "vocal"
                    session.tenant_id = resolved_tenant_id
                    if session.state in ("TRANSFERRED", "CONFIRMED"):
                        return {"result": prompts.VOCAL_RESUME_ALREADY_TERMINATED}
                    events = _get_engine(call_id).handle_message(call_id, user_message)
                    response_text = events[0].text if events else "Je n'ai pas compris"
                    print(f"‚úÖ Tool response: '{response_text}'")
                    return {"result": response_text}
            except LockTimeout:
                logger.warning("[CALL_LOCK_TIMEOUT] tenant_id=%s call_id=%s -> fallback result (√©vite 204)", resolved_tenant_id, call_id[:20])
                try:
                    from backend.engine import _persist_ivr_event
                    _persist_ivr_event(
                        ENGINE.session_store.get_or_create(call_id),
                        "call_lock_timeout",
                        reason="concurrent_webhook",
                    )
                except Exception:
                    pass
                # Ne jamais renvoyer 204 √† Vapi sur /tool ‚Üí silence.
                return JSONResponse({"result": "Un instant, s'il vous pla√Æt."}, status_code=200)
            except Exception as e:
                logger.warning("[CALL_LOCK_WARN] err=%s", e, exc_info=True)
                pass

        session = _get_or_resume_voice_session(resolved_tenant_id, call_id)
        session.channel = "vocal"
        session.tenant_id = resolved_tenant_id
        if session.state in ("TRANSFERRED", "CONFIRMED"):
            return {"result": prompts.VOCAL_RESUME_ALREADY_TERMINATED}
        events = _get_engine(call_id).handle_message(call_id, user_message)
        response_text = events[0].text if events else "Je n'ai pas compris"
        print(f"‚úÖ Tool response: '{response_text}'")
        return {"result": response_text}
        
    except Exception as e:
        print(f"‚ùå Tool error: {e}")
        import traceback
        traceback.print_exc()
        return {"result": "D√©sol√©, une erreur est survenue."}


@router.get("/_health")
async def vapi_internal_health():
    """Health check d√©di√© Vapi (v√©rifier d√©ploiement)."""
    return {"status": "ok", "service": "vapi"}


@router.post("/chat/completions")
async def vapi_custom_llm(request: Request):
    """
    Vapi Custom LLM endpoint
    Vapi envoie les messages ici au lieu d'utiliser Claude/GPT
    Supporte le streaming (SSE) quand stream=true
    
    Int√©grations:
    - M√©moire client (reconnaissance clients r√©currents)
    - Stats pour rapports quotidiens
    """
    t_start = time.time()
    try:
        payload = await request.json()
        headers = request.headers

        # ‚úÖ EXTRACTION STABLE call_id (ordre de priorit√©)
        call_id = None
        if payload.get("call") and payload["call"].get("id"):
            call_id = payload["call"]["id"]
        if not call_id:
            call_id = headers.get("x-vapi-call-id")
        if not call_id:
            call_id = payload.get("conversation_id")
        if not call_id:
            call_id = f"chat-{payload.get('id', 'unknown')}"

        _req_id = str(uuid.uuid4())[:8]
        _source = "body.call.id" if (payload.get("call") and payload["call"].get("id")) else ("header" if headers.get("x-vapi-call-id") else "conversation_id_or_fallback")
        logger.info("session_key_debug", extra={"call_id": call_id, "source": _source})
        logger.info("CHAT_HIT", extra={"call_id": call_id, "request_id": _req_id})
        t1 = log_timer("Payload parsed", t_start)

        print(f"ü§ñ CUSTOM LLM | session_key={call_id} | source={_source}")
        
        # Vapi envoie un tableau de messages (stream: true = SSE obligatoire c√¥t√© backend)
        messages = payload.get("messages", [])
        is_streaming = _parse_stream_flag(payload)
        logger.info(
            "[CHAT_COMPLETIONS] call_id=%s messages_count=%s stream=%s",
            call_id[:24] if call_id else "n/a", len(messages), is_streaming,
        )
        
        # üì± Extraire le num√©ro de t√©l√©phone du client (Vapi le fournit)
        customer_phone = payload.get("call", {}).get("customer", {}).get("number")
        if not customer_phone:
            customer_phone = payload.get("customer", {}).get("number")

        # üéØ DID ‚Üí tenant_id (avant tout event, pour scoping correct)
        from backend.tenant_routing import (
            extract_to_number_from_vapi_payload,
            resolve_tenant_id_from_vocal_call,
        )
        to_number = extract_to_number_from_vapi_payload(payload)
        resolved_tenant_id, route_source = resolve_tenant_id_from_vocal_call(to_number, channel="vocal")
        logger.info(
            "[TENANT_ROUTE] to=%s tenant_id=%s source=%s",
            to_number or "(none)",
            resolved_tenant_id,
            route_source,
        )
        if config.ENABLE_TENANT_ROUTE_MISS_GUARD and to_number and route_source == "default":
            logger.warning("[TENANT_ROUTE_MISS] to=%s tenant_id=%s num√©ro non onboard√©", to_number, resolved_tenant_id)

        print(f"üìû Call ID: {call_id} | Messages: {len(messages)} | Stream: {is_streaming}")
        if customer_phone:
            print(f"üì± Customer phone: {customer_phone}")
        
        # R√©cup√®re le dernier message utilisateur (content peut √™tre string ou liste OpenAI)
        user_message = None
        for msg in reversed(messages):
            if msg.get("role") == "user":
                raw = msg.get("content")
                if isinstance(raw, str):
                    user_message = raw
                elif isinstance(raw, list):
                    user_message = ""
                    for part in raw:
                        if isinstance(part, dict) and part.get("type") == "text":
                            user_message = part.get("text") or ""
                            break
                else:
                    user_message = str(raw) if raw is not None else ""
                break

        t2 = log_timer("Message extracted", t1)
        logger.info(
            "[CHAT_COMPLETIONS] user_message len=%s preview=%s",
            len(user_message or ""), (user_message or "")[:80],
        )
        logger.debug("user message: %r", (user_message or "")[:80])
        
        if not user_message:
            # Premier message ou pas de message user
            response_text = prompts.get_vocal_greeting(config.BUSINESS_NAME)
        else:
            # Traiter via ENGINE
            overlap_handled = False
            response_text = ""
            action_taken = ""

            # Phase 2: PG-first read ‚Äî pas de lock sur /chat/completions.
            # Vapi envoie les tours de fa√ßon s√©quentielle (attend la r√©ponse avant le tour suivant).
            # Un lock bloquait le 2e tour (LockTimeout ‚Üí greeting au lieu de la vraie r√©ponse).
            _call_journal_ensure(resolved_tenant_id, call_id)
            session = _get_or_resume_voice_session(resolved_tenant_id, call_id)
            state_before_turn = getattr(session, "state", "START")

            session.channel = "vocal"
            session.tenant_id = resolved_tenant_id

            # Garde-fou Phase 2: session d√©j√† termin√©e (CONFIRMED/TRANSFERRED) ‚Üí ne pas rouvrir
            if session.state in ("TRANSFERRED", "CONFIRMED"):
                response_text = prompts.VOCAL_RESUME_ALREADY_TERMINATED
                action_taken = "resume_terminal_guard"
                overlap_handled = True

            # P0 Option B: dual-write journal PG (Phase 1)
            _call_journal_ensure(resolved_tenant_id, call_id, state_before_turn)
            _call_journal_user_message(resolved_tenant_id, call_id, user_message or "")

            # üß† Stocker le t√©l√©phone dans la session pour plus tard
            if customer_phone:
                session.customer_phone = customer_phone
            
            # üîÑ RECONSTRUCTION DE L'√âTAT depuis l'historique des messages
            # NOTE: Avec SQLite, cette reconstruction ne devrait plus √™tre n√©cessaire
            # On la garde en fallback si SQLite √©choue
            # Guard: si on VA reconstruire ET qu'on a d√©j√† reconstruit 1 fois ‚Üí transfert (√©vite boucle)
            needs_reconstruct = session.state == "START" and len(messages) > 1 and not session.qualif_data.name
            reconstruct_count = getattr(session, "reconstruct_count", 0)
            if needs_reconstruct and reconstruct_count >= 1:
                logger.warning("[SESSION_RECONSTRUCT] conv_id=%s reconstruct_count=%s -> transfer", call_id, reconstruct_count)
                session.state = "TRANSFERRED"
                response_text = prompts.VOCAL_TRANSFER_COMPLEX
                session.add_message("agent", response_text)
                action_taken = "reconstruct_loop_guard"
                overlap_handled = True  # skip engine processing
            elif needs_reconstruct:
                logger.debug("session in START with history but no data -> reconstruction")
                session = _reconstruct_session_from_history(session, messages, call_id=call_id)
                session.reconstruct_count = 1
            else:
                logger.debug("session loaded OK: state=%s name=%s", session.state, session.qualif_data.name)
            
            t3 = log_timer("Session loaded", t2)
            
            # üß† Check si client r√©current (avant le premier message trait√©)
            if customer_phone:
                try:
                    existing_client = client_memory.get_by_phone(customer_phone)
                    if existing_client:
                        session.client_id = existing_client.id  # pour ivr_events / rapport quotidien
                        if existing_client.total_bookings > 0:
                            greeting = client_memory.get_personalized_greeting(existing_client, channel="vocal")
                            if greeting:
                                logger.debug("returning client detected: %s", existing_client.name)
                except Exception as e:
                    logger.debug("client memory error: %s", e)

            # Input firewall (text-only) : SILENCE / UNCLEAR / TEXT ‚Äî avant tout traitement
            kind, normalized = classify_text_only(user_message or "")
            unclear_count = getattr(session, "unclear_text_count", 0)
            logger.info(
                "decision_in",
                extra={
                    "call_id": call_id,
                    "state_before": session.state,
                    "kind": kind,
                    "raw_len": len((user_message or "")),
                    "normalized_len": len(normalized or ""),
                    "unclear_count": unclear_count,
                },
            )
            logger.info(
                "decision_in_chat",
                extra={
                    "call_id": call_id,
                    "state_before": session.state,
                    "turn_count": getattr(session, "turn_count", 0),
                },
            )

            # Semi-sourd : overlap guard (UNCLEAR/SILENCE pendant TTS = ignor√© ; mots critiques passent)
            # Ne pas r√©initialiser si d√©j√† g√©r√© (reconstruct_loop_guard, resume_terminal_guard)
            if action_taken not in ("reconstruct_loop_guard", "resume_terminal_guard"):
                overlap_handled = False
                response_text = ""
                action_taken = ""
                if _is_agent_speaking(session):
                    # Interruption pendant √©nonciation des cr√©neaux (WAIT_CONFIRM) : "un", "1", "deux" = choix valide
                    if session.state == "WAIT_CONFIRM" and is_critical_token(normalized):
                        overlap_handled = False
                    elif is_critical_overlap(user_message or ""):
                        logger.info(
                            "critical_overlap_allowed",
                            extra={"call_id": call_id, "text_len": len((user_message or "")[:20])},
                        )
                    elif kind in ("UNCLEAR", "SILENCE"):
                        response_text = prompts.MSG_VOCAL_CROSSTALK_ACK
                        action_taken = "overlap_ignored"
                        overlap_handled = True
                        logger.info(
                            "overlap_ignored",
                            extra={"call_id": call_id, "classification": kind, "reason": "agent_speaking"},
                        )
                    elif kind == "TEXT" and len((user_message or "").strip()) < 10:
                        response_text = getattr(
                            prompts, "MSG_OVERLAP_REPEAT_SHORT", "Pardon, pouvez-vous r√©p√©ter ?"
                        )
                        session.add_message("agent", response_text)
                        action_taken = "overlap_repeat"
                        overlap_handled = True
                        logger.info(
                            "overlap_repeat",
                            extra={"call_id": call_id, "text_len": len((user_message or "").strip())},
                        )

            # En annulation : si on va chercher le RDV par nom, envoyer d'abord un message de tenue
            # en stream pour √©viter le "mmm" TTS pendant la latence (recherche Google Calendar).
            cancel_lookup_streaming = (
                is_streaming
                and session.state == "CANCEL_NAME"
                and _looks_like_name_for_cancel(user_message)
            )
            if cancel_lookup_streaming:
                response_text = ""
            else:
                if not overlap_handled:
                    try:
                        if kind == "SILENCE":
                            events = _get_engine(call_id).handle_message(call_id, "")
                            response_text = events[0].text if events else prompts.MSG_EMPTY_MESSAGE
                            action_taken = "silence"
                            _maybe_reset_noise_on_terminal(session, events or [])
                        elif kind == "TEXT":
                            events = _get_engine(call_id).handle_message(call_id, normalized)
                            response_text = events[0].text if events else "Je n'ai pas compris"
                            action_taken = "text"
                            _maybe_reset_noise_on_terminal(session, events or [])
                        else:  # UNCLEAR ‚Äî overlap guard puis crosstalk : ne pas compter overlap comme √©chec
                            now = time.time()
                            last_reply_ts = getattr(session, "last_agent_reply_ts", 0) or 0
                            overlap_window = getattr(config, "OVERLAP_WINDOW_SEC", 1.2)
                            recent_agent = (now - last_reply_ts) < overlap_window
                            if recent_agent:
                                response_text = getattr(
                                    prompts, "MSG_OVERLAP_REPEAT", "Je vous ai entendu en m√™me temps. Pouvez-vous r√©p√©ter maintenant ?"
                                )
                                session.add_message("agent", response_text)
                                action_taken = "overlap_guard"
                            else:
                                raw_len = len((user_message or ""))
                                last_ts = getattr(session, "last_assistant_ts", 0) or 0
                                within_crosstalk_window = (now - last_ts) < getattr(
                                    config, "CROSSTALK_WINDOW_SEC", 5.0
                                )
                                max_crosstalk_len = getattr(config, "CROSSTALK_MAX_RAW_LEN", 40)
                                if within_crosstalk_window and raw_len <= max_crosstalk_len:
                                    response_text = prompts.MSG_VOCAL_CROSSTALK_ACK
                                    action_taken = "ignore_crosstalk"
                                else:
                                    session.unclear_text_count = getattr(session, "unclear_text_count", 0) + 1
                                    count = session.unclear_text_count
                                    if count == 1:
                                        response_text = prompts.MSG_UNCLEAR_1
                                        session.add_message("agent", response_text)
                                        action_taken = "unclear_1"
                                    elif count == 2:
                                        events = ENGINE._trigger_intent_router(
                                            session, "unclear_text_2", user_message or ""
                                        )
                                        response_text = events[0].text if events else prompts.MSG_UNCLEAR_1
                                        action_taken = "unclear_2_intent_router"
                                    else:
                                        session.state = "TRANSFERRED"
                                        response_text = (
                                            prompts.VOCAL_TRANSFER_COMPLEX
                                            if getattr(session, "channel", "") == "vocal"
                                            else prompts.MSG_TRANSFER
                                        )
                                        session.add_message("agent", response_text)
                                        action_taken = "unclear_3_transfer"
                    except Exception as e:
                        print(f"‚ùå ENGINE ERROR: {e}")
                        import traceback
                        traceback.print_exc()
                        response_text = "Excusez-moi, j'ai un petit souci technique. Je vous transf√®re √† un coll√®gue."
                t4 = log_timer("ENGINE processed", t3)
                _log_decision_out(call_id, session, action_taken, response_text)
                if hasattr(ENGINE.session_store, "save"):
                    ENGINE.session_store.save(session)
                # P0 Option B: dual-write ‚Äî message agent + checkpoint
                state_after = getattr(session, "state", "START")
                # Checkpoint sur: changement √©tat, pending_slots, awaiting_confirmation, √©tats critiques
                should_cp = (
                    state_before_turn != state_after
                    or bool(getattr(session, "pending_slots", None))
                    or getattr(session, "awaiting_confirmation", None) is not None
                    or state_after in ("QUALIF_CONTACT", "WAIT_CONFIRM", "CONTACT_CONFIRM")
                )
                _call_journal_agent_response(
                    resolved_tenant_id,
                    call_id,
                    session,
                    response_text,
                    state_before_turn,
                    should_checkpoint=should_cp,
                )
                logger.info(
                    "decision_out_chat",
                    extra={"call_id": call_id, "state_after": getattr(session, "state", "")},
                )
            if not cancel_lookup_streaming:
                print(f"‚úÖ Response: '{response_text[:50]}...' ({len(response_text)} chars)")
                session.last_assistant_ts = time.time()
                session.last_agent_reply_ts = time.time()
                if response_text and response_text.strip():
                    tts_duration = estimate_tts_duration(response_text)
                    session.speaking_until_ts = time.time() + tts_duration
                    logger.info(
                        "agent_speaking",
                        extra={
                            "call_id": call_id,
                            "tts_duration": round(tts_duration, 2),
                            "speaking_until_ts": session.speaking_until_ts,
                        },
                    )
            
            # üìä Enregistrer stats pour rapport (si conversation termin√©e) ‚Äî pas en cancel_lookup_streaming (fait dans le stream)
            if not cancel_lookup_streaming:
                try:
                    if session.state in ["CONFIRMED", "TRANSFERRED"]:
                        intent = "BOOKING" if session.state == "CONFIRMED" else "TRANSFER"
                        outcome = "confirmed" if session.state == "CONFIRMED" else "transferred"
                        duration_ms = int((time.time() - t_start) * 1000)
                        report_generator.record_interaction(
                            call_id=call_id,
                            intent=intent,
                            outcome=outcome,
                            channel="vocal",
                            duration_ms=duration_ms,
                            motif=session.qualif_data.motif if hasattr(session, 'qualif_data') else None,
                            client_name=session.qualif_data.name if hasattr(session, 'qualif_data') else None,
                            client_phone=customer_phone
                        )
                        print(f"üìä Stats recorded: {intent} ‚Üí {outcome}")
                        if session.state == "CONFIRMED" and session.qualif_data.name:
                            try:
                                client = client_memory.get_or_create(
                                    phone=customer_phone,
                                    name=session.qualif_data.name,
                                    email=session.qualif_data.contact if session.qualif_data.contact_type == "email" else None
                                )
                                slot_label = tools_booking.get_label_for_choice(session, session.pending_slot_choice or 1) or "RDV"
                                client_memory.record_booking(
                                    client_id=client.id,
                                    slot_label=slot_label,
                                    motif=session.qualif_data.motif or "consultation"
                                )
                                print(f"üß† Client saved: {client.name} (id={client.id})")
                            except Exception as e:
                                print(f"‚ö†Ô∏è Client save error: {e}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Stats recording error: {e}")
        
        # ‚è±Ô∏è TIMING TOTAL
        total_ms = (time.time() - t_start) * 1000
        print(f"‚úÖ TOTAL LATENCY: {total_ms:.0f}ms")
        
        # Si streaming demand√©, retourner SSE
        if is_streaming:
            async def generate_stream():
                import asyncio
                
                chunk_role = {
                    "id": f"chatcmpl-{call_id}",
                    "object": "chat.completion.chunk",
                    "choices": [{"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}]
                }
                yield f"data: {json.dumps(chunk_role)}\n\n"
                
                stream_response_text = response_text
                if cancel_lookup_streaming:
                    # Envoyer d'abord le message de tenue pour √©viter le "mmm" pendant la recherche du RDV
                    holding = prompts.VOCAL_CANCEL_LOOKUP_HOLDING
                    for i, word in enumerate(holding.split()):
                        content = f" {word}" if i > 0 else word
                        chunk = {
                            "id": f"chatcmpl-{call_id}",
                            "object": "chat.completion.chunk",
                            "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]
                        }
                        yield f"data: {json.dumps(chunk)}\n\n"
                    # Recherche du RDV (bloquant ‚Üí en thread)
                    events = await asyncio.to_thread(_get_engine(call_id).handle_message, call_id, user_message)
                    session_after = ENGINE.session_store.get(call_id)
                    stream_response_text = events[0].text if events else "Je n'ai pas compris"
                    # Stats (m√™me logique qu'en non-streaming)
                    if session_after and session_after.state in ["CONFIRMED", "TRANSFERRED"]:
                        try:
                            intent = "BOOKING" if session_after.state == "CONFIRMED" else "TRANSFER"
                            outcome = "confirmed" if session_after.state == "CONFIRMED" else "transferred"
                            report_generator.record_interaction(
                                call_id=call_id, intent=intent, outcome=outcome, channel="vocal",
                                duration_ms=int((time.time() - t_start) * 1000),
                                motif=getattr(session_after.qualif_data, "motif", None),
                                client_name=getattr(session_after.qualif_data, "name", None),
                                client_phone=customer_phone
                            )
                            if session_after.state == "CONFIRMED" and session_after.qualif_data.name:
                                client = client_memory.get_or_create(
                                    phone=customer_phone,
                                    name=session_after.qualif_data.name,
                                    email=session_after.qualif_data.contact if getattr(session_after.qualif_data, "contact_type", None) == "email" else None
                                )
                                slot_label = tools_booking.get_label_for_choice(session_after, session_after.pending_slot_choice or 1) or "RDV"
                                client_memory.record_booking(client_id=client.id, slot_label=slot_label, motif=session_after.qualif_data.motif or "consultation")
                        except Exception:
                            pass
                
                # Envoyer le contenu (r√©ponse r√©elle) mot par mot
                words = stream_response_text.split()
                for i, word in enumerate(words):
                    content = f" {word}" if i > 0 else word
                    chunk = {
                        "id": f"chatcmpl-{call_id}",
                        "object": "chat.completion.chunk",
                        "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]
                    }
                    yield f"data: {json.dumps(chunk)}\n\n"
                
                chunk_final = {
                    "id": f"chatcmpl-{call_id}",
                    "object": "chat.completion.chunk",
                    "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]
                }
                yield f"data: {json.dumps(chunk_final)}\n\n"
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(
                generate_stream(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive"
                }
            )
        
        # Format OpenAI-compatible (non-streaming), compatibilit√© max + Content-Type strict
        return _make_chat_response(call_id, response_text, is_streaming)

    except Exception as e:
        print(f"‚ùå Custom LLM error: {e}")
        import traceback
        traceback.print_exc()
        try:
            _err_cid = (payload.get("call") or {}).get("id") or "unknown"
        except NameError:
            _err_cid = "unknown"
        try:
            _err_stream = _parse_stream_flag(payload)
        except Exception:
            _err_stream = False
        logger.warning(
            "[CHAT_COMPLETIONS] exception call_id=%s stream=%s err=%s",
            _err_cid[:24] if _err_cid else "n/a", _err_stream, str(e),
        )
        _err_msg = "D√©sol√©, une erreur est survenue."
        return _make_chat_response(_err_cid, _err_msg, _err_stream)


@router.get("/health")
async def vapi_health():
    return {"status": "ok", "service": "voice"}


@router.get("/test")
async def vapi_test():
    try:
        events = _get_engine("test").handle_message("test", "bonjour")
        if events:
            return {"status": "ok", "response": events[0].text}
        return {"status": "error"}
    except Exception as e:
        return {"status": "error", "error": str(e)}
