# backend/routes/voice.py
"""
Route pour le canal Voix (Vapi) - DEBUG COMPLET + TIMERS
Avec m√©moire client et stats pour rapports.
"""

from fastapi import APIRouter, Request
import logging
import json
import time

from backend.engine import ENGINE
from backend import prompts
from backend.client_memory import get_client_memory
from backend.reports import get_report_generator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Instances singleton
client_memory = get_client_memory()
report_generator = get_report_generator()


def _reconstruct_session_from_history(session, messages: list):
    """
    Reconstruit l'√©tat de la session depuis l'historique des messages.
    N√©cessaire si la session en m√©moire a √©t√© perdue.
    
    Analyse les messages assistant pour d√©terminer o√π on en est.
    """
    # Patterns pour d√©tecter l'√©tat
    patterns = {
        "QUALIF_NAME": ["c'est √† quel nom", "quel nom", "votre nom"],
        "QUALIF_PREF": ["matin ou l'apr√®s-midi", "matin ou apr√®s-midi", "pr√©f√©rez"],
        "QUALIF_CONTACT": ["num√©ro de t√©l√©phone", "t√©l√©phone", "email"],
        "WAIT_CONFIRM": ["cr√©neaux disponibles", "premier choix", "lequel vous convient"],
        "CONFIRMED": ["rendez-vous est confirm√©", "c'est tout bon"],
    }
    
    # Parcourir les messages pour trouver le dernier √©tat
    last_assistant_msg = ""
    for msg in reversed(messages):
        if msg.get("role") == "assistant":
            last_assistant_msg = msg.get("content", "").lower()
            break
    
    # Extraire le nom si on l'a demand√© puis re√ßu une r√©ponse
    for i, msg in enumerate(messages):
        if msg.get("role") == "assistant":
            content = msg.get("content", "").lower()
            if any(p in content for p in patterns["QUALIF_NAME"]):
                # Le message suivant devrait √™tre le nom
                if i + 1 < len(messages) and messages[i + 1].get("role") == "user":
                    potential_name = messages[i + 1].get("content", "").strip()
                    if len(potential_name) >= 2 and len(potential_name) <= 50:
                        session.qualif_data.name = potential_name
                        print(f"üîÑ Extracted name from history: {potential_name}")
    
    # D√©terminer l'√©tat actuel bas√© sur le dernier message assistant
    for state, state_patterns in patterns.items():
        if any(p in last_assistant_msg for p in state_patterns):
            session.state = state
            print(f"üîÑ Detected state from history: {state}")
            break
    
    return session


def log_timer(label: str, start: float) -> float:
    """Log le temps √©coul√© et retourne le nouveau timestamp."""
    now = time.time()
    elapsed_ms = (now - start) * 1000
    print(f"‚è±Ô∏è {label}: {elapsed_ms:.0f}ms")
    return now

router = APIRouter(prefix="/api/vapi", tags=["voice"])


@router.post("/webhook")
async def vapi_webhook(request: Request):
    """
    Webhook Vapi - DEBUG COMPLET + TIMERS
    """
    t_start = time.time()
    
    try:
        payload = await request.json()
        t1 = log_timer("Payload parsed", t_start)
        
        message = payload.get("message", {})
        message_type = message.get("type", "NO_TYPE")
        call_id = payload.get("call", {}).get("id", "unknown")
        
        print(f"üîî WEBHOOK | type={message_type} | call={call_id}")
        
        # assistant-request
        if message_type == "assistant-request":
            print("‚úÖ Returning {} for assistant-request")
            return {}
        
        # ACCEPTE TOUS LES MESSAGES AVEC DU TEXTE
        user_text = message.get("content") or message.get("transcript") or ""
        t2 = log_timer("Message extracted", t1)
        
        if user_text and user_text.strip():
            print(f"üí¨ User: '{user_text}'")
            
            session = ENGINE.session_store.get_or_create(call_id)
            session.channel = "vocal"
            t3 = log_timer("Session loaded", t2)
            
            events = ENGINE.handle_message(call_id, user_text)
            t4 = log_timer("ENGINE processed", t3)
            
            response_text = events[0].text if events else "Je n'ai pas compris"
            
            # ‚è±Ô∏è TIMING TOTAL
            total_ms = (time.time() - t_start) * 1000
            print(f"‚úÖ TOTAL: {total_ms:.0f}ms | Response: '{response_text[:50]}...'")
            
            return {"content": response_text}
        
        print(f"‚ö†Ô∏è No user text found")
        return {}
        
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()
        return {"content": "D√©sol√©, une erreur est survenue."}


@router.post("/tool")
async def vapi_tool(request: Request):
    """
    Endpoint pour Vapi Tools/Functions.
    Claude appelle ce tool pour obtenir les r√©ponses.
    """
    try:
        payload = await request.json()
        
        print(f"üîßüîßüîß TOOL APPEL√â üîßüîßüîß")
        print(f"üì¶ Payload: {json.dumps(payload, indent=2, ensure_ascii=False)}")
        
        # Extraire le message utilisateur
        user_message = payload.get("parameters", {}).get("user_message", "")
        call_id = payload.get("call", {}).get("id", "unknown")
        
        print(f"üìù User message: '{user_message}'")
        print(f"üìû Call ID: {call_id}")
        
        if not user_message:
            return {"result": "Je n'ai pas compris. Pouvez-vous r√©p√©ter ?"}
        
        # Session vocale
        session = ENGINE.session_store.get_or_create(call_id)
        session.channel = "vocal"
        
        # Traiter
        events = ENGINE.handle_message(call_id, user_message)
        response_text = events[0].text if events else "Je n'ai pas compris"
        
        print(f"‚úÖ Tool response: '{response_text}'")
        
        return {"result": response_text}
        
    except Exception as e:
        print(f"‚ùå Tool error: {e}")
        import traceback
        traceback.print_exc()
        return {"result": "D√©sol√©, une erreur est survenue."}


@router.post("/chat/completions")
async def vapi_custom_llm(request: Request):
    """
    Vapi Custom LLM endpoint
    Vapi envoie les messages ici au lieu d'utiliser Claude/GPT
    Supporte le streaming (SSE) quand stream=true
    
    Int√©grations:
    - M√©moire client (reconnaissance clients r√©currents)
    - Stats pour rapports quotidiens
    """
    from fastapi.responses import StreamingResponse
    
    # ‚è±Ô∏è TIMING START
    t_start = time.time()
    
    try:
        payload = await request.json()
        t1 = log_timer("Payload parsed", t_start)
        
        print(f"ü§ñ CUSTOM LLM | Payload size: {len(str(payload))} chars")
        
        # Vapi envoie un tableau de messages
        messages = payload.get("messages", [])
        call_id = payload.get("call", {}).get("id") or payload.get("call_id", "unknown")
        is_streaming = payload.get("stream", False)
        
        # üì± Extraire le num√©ro de t√©l√©phone du client (Vapi le fournit)
        customer_phone = payload.get("call", {}).get("customer", {}).get("number")
        if not customer_phone:
            customer_phone = payload.get("customer", {}).get("number")
        
        print(f"üìû Call ID: {call_id} | Messages: {len(messages)} | Stream: {is_streaming}")
        if customer_phone:
            print(f"üì± Customer phone: {customer_phone}")
        
        # R√©cup√®re le dernier message utilisateur
        user_message = None
        for msg in reversed(messages):
            if msg.get("role") == "user":
                user_message = msg.get("content")
                break
        
        t2 = log_timer("Message extracted", t1)
        print(f"üí¨ User: '{user_message}'")
        
        if not user_message:
            # Premier message ou pas de message user
            response_text = prompts.MSG_WELCOME
            print(f"‚úÖ Welcome message")
        else:
            # Traiter via ENGINE
            session = ENGINE.session_store.get_or_create(call_id)
            session.channel = "vocal"
            
            # üß† Stocker le t√©l√©phone dans la session pour plus tard
            if customer_phone:
                session.customer_phone = customer_phone
            
            # üîÑ RECONSTRUCTION DE L'√âTAT depuis l'historique des messages
            # N√©cessaire si la session en m√©moire a √©t√© perdue (red√©marrage Railway, etc.)
            if session.state == "START" and len(messages) > 1:
                session = _reconstruct_session_from_history(session, messages)
                print(f"üîÑ Session reconstructed: state={session.state}, name={session.qualif_data.name}")
            
            t3 = log_timer("Session loaded", t2)
            
            # üß† Check si client r√©current (avant le premier message trait√©)
            if customer_phone and session.state == "START" and len(messages) <= 1:
                try:
                    existing_client = client_memory.get_by_phone(customer_phone)
                    if existing_client and existing_client.total_bookings > 0:
                        # Client r√©current d√©tect√© !
                        greeting = client_memory.get_personalized_greeting(existing_client, channel="vocal")
                        if greeting:
                            print(f"üß† Returning client detected: {existing_client.name}")
                            # On pourrait utiliser ce greeting, mais pour l'instant on log juste
                            # Le flow normal continue
                except Exception as e:
                    print(f"‚ö†Ô∏è Client memory error: {e}")
            
            events = ENGINE.handle_message(call_id, user_message)
            t4 = log_timer("ENGINE processed", t3)
            
            response_text = events[0].text if events else "Je n'ai pas compris"
            print(f"‚úÖ Response: '{response_text[:50]}...' ({len(response_text)} chars)")
            
            # üìä Enregistrer stats pour rapport (si conversation termin√©e)
            try:
                if session.state in ["CONFIRMED", "TRANSFERRED"]:
                    intent = "BOOKING" if session.state == "CONFIRMED" else "TRANSFER"
                    outcome = "confirmed" if session.state == "CONFIRMED" else "transferred"
                    duration_ms = int((time.time() - t_start) * 1000)
                    
                    report_generator.record_interaction(
                        call_id=call_id,
                        intent=intent,
                        outcome=outcome,
                        channel="vocal",
                        duration_ms=duration_ms,
                        motif=session.qualif_data.motif if hasattr(session, 'qualif_data') else None,
                        client_name=session.qualif_data.name if hasattr(session, 'qualif_data') else None,
                        client_phone=customer_phone
                    )
                    print(f"üìä Stats recorded: {intent} ‚Üí {outcome}")
                    
                    # üß† Enregistrer le client si booking confirm√©
                    if session.state == "CONFIRMED" and session.qualif_data.name:
                        try:
                            client = client_memory.get_or_create(
                                phone=customer_phone,
                                name=session.qualif_data.name,
                                email=session.qualif_data.contact if session.qualif_data.contact_type == "email" else None
                            )
                            # Enregistrer le booking dans l'historique client
                            slot_label = session.pending_slot_labels[0] if session.pending_slot_labels else "RDV"
                            client_memory.record_booking(
                                client_id=client.id,
                                slot_label=slot_label,
                                motif=session.qualif_data.motif or "consultation"
                            )
                            print(f"üß† Client saved: {client.name} (id={client.id})")
                        except Exception as e:
                            print(f"‚ö†Ô∏è Client save error: {e}")
            except Exception as e:
                print(f"‚ö†Ô∏è Stats recording error: {e}")
        
        # ‚è±Ô∏è TIMING TOTAL
        total_ms = (time.time() - t_start) * 1000
        print(f"‚úÖ TOTAL LATENCY: {total_ms:.0f}ms")
        
        # Si streaming demand√©, retourner SSE
        if is_streaming:
            async def generate_stream():
                import asyncio
                
                # Premier chunk : r√¥le assistant
                chunk_role = {
                    "id": f"chatcmpl-{call_id}",
                    "object": "chat.completion.chunk",
                    "choices": [{
                        "index": 0,
                        "delta": {"role": "assistant"},
                        "finish_reason": None
                    }]
                }
                yield f"data: {json.dumps(chunk_role)}\n\n"
                
                # Envoyer le contenu mot par mot
                words = response_text.split()
                for i, word in enumerate(words):
                    # Ajouter espace sauf pour le premier mot
                    content = f" {word}" if i > 0 else word
                    chunk = {
                        "id": f"chatcmpl-{call_id}",
                        "object": "chat.completion.chunk",
                        "choices": [{
                            "index": 0,
                            "delta": {"content": content},
                            "finish_reason": None
                        }]
                    }
                    yield f"data: {json.dumps(chunk)}\n\n"
                
                # Chunk final
                chunk_final = {
                    "id": f"chatcmpl-{call_id}",
                    "object": "chat.completion.chunk",
                    "choices": [{
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }]
                }
                yield f"data: {json.dumps(chunk_final)}\n\n"
                yield "data: [DONE]\n\n"
            
            return StreamingResponse(
                generate_stream(),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive"
                }
            )
        
        # Format OpenAI-compatible (non-streaming)
        return {
            "id": f"chatcmpl-{call_id}",
            "object": "chat.completion",
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }]
        }
        
    except Exception as e:
        print(f"‚ùå Custom LLM error: {e}")
        import traceback
        traceback.print_exc()
        return {
            "choices": [{
                "message": {
                    "role": "assistant",
                    "content": "D√©sol√©, une erreur est survenue."
                }
            }]
        }


@router.get("/health")
async def vapi_health():
    return {"status": "ok", "service": "voice"}


@router.get("/test")
async def vapi_test():
    try:
        events = ENGINE.handle_message("test", "bonjour")
        if events:
            return {"status": "ok", "response": events[0].text}
        return {"status": "error"}
    except Exception as e:
        return {"status": "error", "error": str(e)}
